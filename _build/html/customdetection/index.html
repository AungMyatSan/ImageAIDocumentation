

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Custom Object Detection: Training and Inference &mdash; ImageAI 2.1.5 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Custom Training: Prediction" href="../custom/index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> ImageAI
          

          
          </a>

          
            
            
              <div class="version">
                3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../prediction/index.html">Prediction Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../detection/index.html">Detection Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../video/index.html">Video and Live-Feed Detection and Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom/index.html">Custom Training: Prediction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Custom Object Detection: Training and Inference</a><ul class="simple">
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ImageAI</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Custom Object Detection: Training and Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/customdetection/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="custom-object-detection-training-and-inference">
<h1>Custom Object Detection: Training and Inference<a class="headerlink" href="#custom-object-detection-training-and-inference" title="Permalink to this headline">¶</a></h1>
<div class="figure align-default">
<img alt="../_images/image7.jpg" src="../_images/image7.jpg" />
</div>
<p><strong>ImageAI</strong> provides the simple and powerful approach to training <strong>custom object detection</strong> models using the <strong>YOLOv3</strong> architeture. This allows you to train your own model on any set of images that corresponds to any type of object of interest.</p>
<p>You can use your trained detection models to detect objects in images, videos and perform video analysis.</p>
<p><strong>======= imageai.Detection.Custom.DetectionModelTrainer =======</strong></p>
<p>This is the Detection Model training class, which allows you to train object detection models on image datasets that are in Pascal VOC annotation format, using the YOLOv3.
The training process generates a JSON file that maps the objects names in your image dataset and the detection anchors, as well as creates lots of models.</p>
<p>To get started, you need prepare your dataset in the <strong>Pascal VOC Format</strong> and organize it as detailed below:</p>
<p>– Decide the type of object(s) you want to detect and collect about 200 (minimum recommendation) or more picture of each of the object(s)</p>
<p>– Once you have collected the images, you need to annotate the object(s) in the images. You can use a tool like  <a class="reference external" href="https://github.com/tzutalin/labelImg">LabelIMG</a> to generate the annotations for your images.</p>
<p>– Once you have the annotations for all your images, create a folder for your dataset <strong>(E.g headsets)</strong> and in this parent folder, create child folders <strong>train</strong> and <strong>validation</strong></p>
<p>– In the <strong>train</strong> folder, create <strong>images</strong> and <strong>annotations</strong> sub-folders. Put about 70-80% of your dataset of each object’s images in the <strong>images</strong> folder and put the corresponding annotations for these images in the <strong>annotations</strong> folder.</p>
<p>– In the <strong>validation</strong> folder, create <strong>images</strong> and <strong>annotations</strong> sub-folders. Put the rest of your dataset images in the <strong>images</strong> folder and put the corresponding annotations for these images in the <strong>annotations</strong> folder.</p>
<p>– Once you have done this, the structure of your image dataset folder should look like below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">train</span>    <span class="o">&gt;&gt;</span> <span class="n">images</span>       <span class="o">&gt;&gt;</span> <span class="n">img_1</span><span class="o">.</span><span class="n">jpg</span>  <span class="p">(</span><span class="n">shows</span> <span class="n">Object_1</span><span class="p">)</span>
            <span class="o">&gt;&gt;</span> <span class="n">images</span>       <span class="o">&gt;&gt;</span> <span class="n">img_2</span><span class="o">.</span><span class="n">jpg</span>  <span class="p">(</span><span class="n">shows</span> <span class="n">Object_2</span><span class="p">)</span>
            <span class="o">&gt;&gt;</span> <span class="n">images</span>       <span class="o">&gt;&gt;</span> <span class="n">img_3</span><span class="o">.</span><span class="n">jpg</span>  <span class="p">(</span><span class="n">shows</span> <span class="n">Object_1</span><span class="p">,</span> <span class="n">Object_3</span> <span class="ow">and</span> <span class="n">Object_n</span><span class="p">)</span>
            <span class="o">&gt;&gt;</span> <span class="n">annotations</span>  <span class="o">&gt;&gt;</span> <span class="n">img_1</span><span class="o">.</span><span class="n">xml</span>  <span class="p">(</span><span class="n">describes</span> <span class="n">Object_1</span><span class="p">)</span>
            <span class="o">&gt;&gt;</span> <span class="n">annotations</span>  <span class="o">&gt;&gt;</span> <span class="n">img_2</span><span class="o">.</span><span class="n">xml</span>  <span class="p">(</span><span class="n">describes</span> <span class="n">Object_2</span><span class="p">)</span>
            <span class="o">&gt;&gt;</span> <span class="n">annotations</span>  <span class="o">&gt;&gt;</span> <span class="n">img_3</span><span class="o">.</span><span class="n">xml</span>  <span class="p">(</span><span class="n">describes</span> <span class="n">Object_1</span><span class="p">,</span> <span class="n">Object_3</span> <span class="ow">and</span> <span class="n">Object_n</span><span class="p">)</span>

<span class="o">&gt;&gt;</span> <span class="n">validation</span>   <span class="o">&gt;&gt;</span> <span class="n">images</span>       <span class="o">&gt;&gt;</span> <span class="n">img_151</span><span class="o">.</span><span class="n">jpg</span> <span class="p">(</span><span class="n">shows</span> <span class="n">Object_1</span><span class="p">,</span> <span class="n">Object_3</span> <span class="ow">and</span> <span class="n">Object_n</span><span class="p">)</span>
                <span class="o">&gt;&gt;</span> <span class="n">images</span>       <span class="o">&gt;&gt;</span> <span class="n">img_152</span><span class="o">.</span><span class="n">jpg</span> <span class="p">(</span><span class="n">shows</span> <span class="n">Object_2</span><span class="p">)</span>
                <span class="o">&gt;&gt;</span> <span class="n">images</span>       <span class="o">&gt;&gt;</span> <span class="n">img_153</span><span class="o">.</span><span class="n">jpg</span> <span class="p">(</span><span class="n">shows</span> <span class="n">Object_1</span><span class="p">)</span>
                <span class="o">&gt;&gt;</span> <span class="n">annotations</span>  <span class="o">&gt;&gt;</span> <span class="n">img_151</span><span class="o">.</span><span class="n">xml</span> <span class="p">(</span><span class="n">describes</span> <span class="n">Object_1</span><span class="p">,</span> <span class="n">Object_3</span> <span class="ow">and</span> <span class="n">Object_n</span><span class="p">)</span>
                <span class="o">&gt;&gt;</span> <span class="n">annotations</span>  <span class="o">&gt;&gt;</span> <span class="n">img_152</span><span class="o">.</span><span class="n">xml</span> <span class="p">(</span><span class="n">describes</span> <span class="n">Object_2</span><span class="p">)</span>
                <span class="o">&gt;&gt;</span> <span class="n">annotations</span>  <span class="o">&gt;&gt;</span> <span class="n">img_153</span><span class="o">.</span><span class="n">xml</span> <span class="p">(</span><span class="n">describes</span> <span class="n">Object_1</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>You can train your custom detection model completely from scratch or use transfer learning (recommended for better accuracy) from a pre-trained YOLOv3 model. Also, we have provided a sample annotated Hololens and Headsets (Hololens and Oculus) dataset for you to train with. Download the pre-trained YOLOv3 model and the sample datasets in the link below</p></li>
</ul>
<p><a class="reference external" href="https://github.com/OlafenwaMoses/ImageAI/releases/tag/essential-v4">Sample dataset and pre-trained YOLOv3</a></p>
<ul>
<li><p>For the purpose of training your detection model, we advice that you have the <strong>Tensorflow-GPU v1.13.1</strong> installed to avoid errors:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip3</span> <span class="n">install</span> <span class="n">tensorflow</span><span class="o">-</span><span class="n">gpu</span><span class="o">==</span><span class="mf">1.13</span><span class="o">.</span><span class="mi">1</span>
</pre></div>
</div>
</li>
</ul>
<p>Below is the code to train new detection models on your dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imageai.Detection.Custom</span> <span class="kn">import</span> <span class="n">DetectionModelTrainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">DetectionModelTrainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">setModelTypeAsYOLOv3</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">setDataDirectory</span><span class="p">(</span><span class="n">data_directory</span><span class="o">=</span><span class="s2">&quot;hololens&quot;</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">setTrainConfig</span><span class="p">(</span><span class="n">object_names_array</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;hololens&quot;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_experiments</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">train_from_pretrained_model</span><span class="o">=</span><span class="s2">&quot;pretrained-yolov3.h5&quot;</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">trainModel</span><span class="p">()</span>
</pre></div>
</div>
<p>In the first 2 lines, we imported the <strong>DetectionModelTrainer</strong> class and created an instance of it</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imageai.Detection.Custom</span> <span class="kn">import</span> <span class="n">DetectionModelTrainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">DetectionModelTrainer</span><span class="p">()</span>
</pre></div>
</div>
<p>Then we called the following functions</p>
<ul>
<li><p><strong>.setModelTypeAsYOLOv3()</strong> , This function sets the model type of the object detection training instance to the <strong>YOLOv3</strong> model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">setModelTypeAsYOLOv3</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p><strong>.trainer.setDataDirectory()</strong> , This function is sets the path to your dataset’s folder:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">setDataDirectory</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<p>– <em>parameter</em> <strong>data_directory</strong> (required) : This is the path to your dataset folder.</p>
<ul>
<li><p><strong>.trainer.setTrainConfig()</strong> , This function sets the properties for the training instances:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">setTrainConfig</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<p>– <em>parameter</em> <strong>object_names_array</strong> (required) : This is a list of the names of all the different objects in your dataset.</p>
<p>– <em>parameter</em> <strong>batch_size</strong> (optional) : This is the batch size for the training instance.</p>
<p>– <em>parameter</em> <strong>num_experiments</strong> (required) : Also known as epochs, it is the number of times the network will train on all the training.</p>
<p>– <em>parameter</em> <strong>train_from_pretrained_model</strong> (optional) : This is used to perform transfer learning by specifying the path to a pre-trained YOLOv3 model</p>
<p>When you run the training code, <strong>ImageAI</strong> will perform the following actions:</p>
<ul class="simple">
<li><p>generate a <strong>detection_config.json</strong> in the <em>dataset_folder/json</em> folder. Please note that the <strong>JSON</strong> file generated in a training session can only be used with the <strong>detection models</strong> saved in the training session.</p></li>
<li><p>saves the <strong>Tensorboard</strong> report for the training in the <em>dataet_folder/logs</em> folder.</p></li>
<li><p>saves new models n the <em>dataset_folder/models</em> folder as the training loss reduces.</p></li>
</ul>
<p>As the training progresses, the information displayed in the terminal will look similar to the sample below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="n">TensorFlow</span> <span class="n">backend</span><span class="o">.</span>
<span class="n">Generating</span> <span class="n">anchor</span> <span class="n">boxes</span> <span class="k">for</span> <span class="n">training</span> <span class="n">images</span> <span class="ow">and</span> <span class="n">annotation</span><span class="o">...</span>
<span class="n">Average</span> <span class="n">IOU</span> <span class="k">for</span> <span class="mi">9</span> <span class="n">anchors</span><span class="p">:</span> <span class="mf">0.78</span>
<span class="n">Anchor</span> <span class="n">Boxes</span> <span class="n">generated</span><span class="o">.</span>
<span class="n">Detection</span> <span class="n">configuration</span> <span class="n">saved</span> <span class="ow">in</span>  <span class="n">hololens</span><span class="o">/</span><span class="n">json</span><span class="o">/</span><span class="n">detection_config</span><span class="o">.</span><span class="n">json</span>
<span class="n">Training</span> <span class="n">on</span><span class="p">:</span>        <span class="p">[</span><span class="s1">&#39;hololens&#39;</span><span class="p">]</span>
<span class="n">Training</span> <span class="k">with</span> <span class="n">Batch</span> <span class="n">Size</span><span class="p">:</span>  <span class="mi">4</span>
<span class="n">Number</span> <span class="n">of</span> <span class="n">Experiments</span><span class="p">:</span>  <span class="mi">200</span>

<span class="n">Epoch</span> <span class="mi">1</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">480</span><span class="o">/</span><span class="mi">480</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">395</span><span class="n">s</span> <span class="mi">823</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">36.9000</span> <span class="o">-</span> <span class="n">yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">3.2970</span> <span class="o">-</span> <span class="n">yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">9.4923</span> <span class="o">-</span> <span class="n">yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">24.1107</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">15.6321</span> <span class="o">-</span> <span class="n">val_yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">2.0275</span> <span class="o">-</span> <span class="n">val_yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">6.4191</span> <span class="o">-</span> <span class="n">val_yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">7.1856</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">480</span><span class="o">/</span><span class="mi">480</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">293</span><span class="n">s</span> <span class="mi">610</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">11.9330</span> <span class="o">-</span> <span class="n">yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">1.3968</span> <span class="o">-</span> <span class="n">yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">4.2894</span> <span class="o">-</span> <span class="n">yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">6.2468</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">7.9868</span> <span class="o">-</span> <span class="n">val_yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">1.7054</span> <span class="o">-</span> <span class="n">val_yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">2.9156</span> <span class="o">-</span> <span class="n">val_yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">3.3657</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">480</span><span class="o">/</span><span class="mi">480</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">293</span><span class="n">s</span> <span class="mi">610</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">7.1228</span> <span class="o">-</span> <span class="n">yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">1.0583</span> <span class="o">-</span> <span class="n">yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">2.2863</span> <span class="o">-</span> <span class="n">yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">3.7782</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">6.4964</span> <span class="o">-</span> <span class="n">val_yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">1.1391</span> <span class="o">-</span> <span class="n">val_yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">2.2058</span> <span class="o">-</span> <span class="n">val_yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">3.1514</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">480</span><span class="o">/</span><span class="mi">480</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">297</span><span class="n">s</span> <span class="mi">618</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">5.5802</span> <span class="o">-</span> <span class="n">yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">0.9742</span> <span class="o">-</span> <span class="n">yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">1.8916</span> <span class="o">-</span> <span class="n">yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">2.7144</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">6.4275</span> <span class="o">-</span> <span class="n">val_yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">1.6153</span> <span class="o">-</span> <span class="n">val_yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">2.1203</span> <span class="o">-</span> <span class="n">val_yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">2.6919</span>
<span class="n">Epoch</span> <span class="mi">5</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">480</span><span class="o">/</span><span class="mi">480</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">295</span><span class="n">s</span> <span class="mi">615</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">4.8717</span> <span class="o">-</span> <span class="n">yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">0.7568</span> <span class="o">-</span> <span class="n">yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">1.6641</span> <span class="o">-</span> <span class="n">yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">2.4508</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">6.3723</span> <span class="o">-</span> <span class="n">val_yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">1.6434</span> <span class="o">-</span> <span class="n">val_yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">2.1188</span> <span class="o">-</span> <span class="n">val_yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">2.6101</span>
<span class="n">Epoch</span> <span class="mi">6</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">480</span><span class="o">/</span><span class="mi">480</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">300</span><span class="n">s</span> <span class="mi">624</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">4.7989</span> <span class="o">-</span> <span class="n">yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">0.8708</span> <span class="o">-</span> <span class="n">yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">1.6683</span> <span class="o">-</span> <span class="n">yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">2.2598</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">5.8672</span> <span class="o">-</span> <span class="n">val_yolo_layer_1_loss</span><span class="p">:</span> <span class="mf">1.2349</span> <span class="o">-</span> <span class="n">val_yolo_layer_2_loss</span><span class="p">:</span> <span class="mf">2.0504</span> <span class="o">-</span> <span class="n">val_yolo_layer_3_loss</span><span class="p">:</span> <span class="mf">2.5820</span>
<span class="n">Epoch</span> <span class="mi">7</span><span class="o">/</span><span class="mi">200</span>
</pre></div>
</div>
<p>After training is completed, you can evaluate the <strong>mAP</strong> score of your saved models in order to pick the one with the most accurate results.</p>
<p>To do this, simply run the code below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imageai.Detection.Custom</span> <span class="kn">import</span> <span class="n">DetectionModelTrainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">DetectionModelTrainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">setModelTypeAsYOLOv3</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">setDataDirectory</span><span class="p">(</span><span class="n">data_directory</span><span class="o">=</span><span class="s2">&quot;hololens&quot;</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluateModel</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;hololens/models&quot;</span><span class="p">,</span> <span class="n">json_path</span><span class="o">=</span><span class="s2">&quot;hololens/json/detection_config.json&quot;</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">object_threshold</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">nms_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code is similar to our training code, except for the line where we called the <strong>evaluateModel()</strong> function. See details on the function below.</p>
<ul>
<li><p><strong>.trainer.evaluateModel()</strong> , This function allows you to compute and obtain the <strong>mAP</strong> of your saved model(s) based on criterias such as <strong>IoU</strong> and <strong>confidence score</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">setTrainConfig</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<p>– <em>parameter</em> <strong>model_path</strong> (required) : This can be the path to a single model or the folder containing your saved models</p>
<p>– <em>parameter</em> <strong>json_path</strong> (required) : This is the <strong>detection_config.json</strong> generated during the training that saved the models.</p>
<p>– <em>parameter</em> <strong>iou_threshold</strong> (optional) : This is used to set the desired <strong>Intersection over Union</strong> for the <strong>mAP</strong> evaluation.</p>
<p>– <em>parameter</em> <strong>object_threshold</strong> (optional) : This is used to set the minimum <strong>confidence score</strong> for the <strong>mAP</strong> evaluation.</p>
<p>– <em>parameter</em> <strong>nms_threshold</strong> (optional) : This is used to set the minimum <strong>Non-maximum Suppression</strong> value for the <strong>mAP</strong> evaluation.</p>
<p>When you run the above code, you get a result similar to the one below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[{</span>
    <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hololens&#39;</span><span class="p">:</span> <span class="mf">0.9231334437735249</span><span class="p">},</span>
    <span class="s1">&#39;map&#39;</span><span class="p">:</span> <span class="mf">0.9231334437735249</span><span class="p">,</span>
    <span class="s1">&#39;model_file&#39;</span><span class="p">:</span> <span class="s1">&#39;hololens/models/detection_model-ex-07--loss-4.42.h5&#39;</span><span class="p">,</span>
    <span class="s1">&#39;using_iou&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_non_maximum_suppression&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_object_threshold&#39;</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hololens&#39;</span><span class="p">:</span> <span class="mf">0.9725334437735249</span><span class="p">},</span>
    <span class="s1">&#39;map&#39;</span><span class="p">:</span> <span class="mf">0.97251334437735249</span><span class="p">,</span>
    <span class="s1">&#39;model_file&#39;</span><span class="p">:</span> <span class="s1">&#39;hololens/models/detection_model-ex-10--loss-3.95.h5&#39;</span><span class="p">,</span>
    <span class="s1">&#39;using_iou&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_non_maximum_suppression&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_object_threshold&#39;</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hololens&#39;</span><span class="p">:</span> <span class="mf">0.92041334437735249</span><span class="p">},</span>
    <span class="s1">&#39;map&#39;</span><span class="p">:</span> <span class="mf">0.92041334437735249</span><span class="p">,</span>
    <span class="s1">&#39;model_file&#39;</span><span class="p">:</span> <span class="s1">&#39;hololens/models/detection_model-ex-05--loss-5.26.h5&#39;</span><span class="p">,</span>
    <span class="s1">&#39;using_iou&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_non_maximum_suppression&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_object_threshold&#39;</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hololens&#39;</span><span class="p">:</span> <span class="mf">0.81201334437735249</span><span class="p">},</span>
    <span class="s1">&#39;map&#39;</span><span class="p">:</span> <span class="mf">0.81201334437735249</span><span class="p">,</span>
    <span class="s1">&#39;model_file&#39;</span><span class="p">:</span> <span class="s1">&#39;hololens/models/detection_model-ex-03--loss-6.44.h5&#39;</span><span class="p">,</span>
    <span class="s1">&#39;using_iou&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_non_maximum_suppression&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_object_threshold&#39;</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hololens&#39;</span><span class="p">:</span> <span class="mf">0.94311334437735249</span><span class="p">},</span>
    <span class="s1">&#39;map&#39;</span><span class="p">:</span> <span class="mf">0.94311334437735249</span><span class="p">,</span>
    <span class="s1">&#39;model_file&#39;</span><span class="p">:</span> <span class="s1">&#39;hololens/models/detection_model-ex-18--loss-2.96.h5&#39;</span><span class="p">,</span>
    <span class="s1">&#39;using_iou&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_non_maximum_suppression&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_object_threshold&#39;</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hololens&#39;</span><span class="p">:</span> <span class="mf">0.94041334437735249</span><span class="p">},</span>
    <span class="s1">&#39;map&#39;</span><span class="p">:</span> <span class="mf">0.94041334437735249</span><span class="p">,</span>
    <span class="s1">&#39;model_file&#39;</span><span class="p">:</span> <span class="s1">&#39;hololens/models/detection_model-ex-17--loss-3.10.h5&#39;</span><span class="p">,</span>
    <span class="s1">&#39;using_iou&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_non_maximum_suppression&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_object_threshold&#39;</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hololens&#39;</span><span class="p">:</span> <span class="mf">0.97251334437735249</span><span class="p">},</span>
    <span class="s1">&#39;map&#39;</span><span class="p">:</span> <span class="mf">0.97251334437735249</span><span class="p">,</span>
    <span class="s1">&#39;model_file&#39;</span><span class="p">:</span> <span class="s1">&#39;hololens/models/detection_model-ex-08--loss-4.16.h5&#39;</span><span class="p">,</span>
    <span class="s1">&#39;using_iou&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_non_maximum_suppression&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;using_object_threshold&#39;</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p><strong>======= imageai.Detection.Custom.CustomObjectDetection =======</strong></p>
<p><strong>CustomObjectDetection</strong> class provides very convenient and powerful methods to perform object detection on images and extract each object from the image using your own custom <strong>YOLOv3 model</strong> and the corresponding <strong>detection_config.json</strong> generated during the training.</p>
<p>To test the custom object detection, you can download a sample custom model we have trained to detect the Hololens headset and its <strong>detection_config.json</strong> file via the links below:</p>
<p><a class="reference external" href="https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/hololens-ex-60--loss-2.76.h5">Hololens Detection Model</a></p>
<p><a class="reference external" href="https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/detection_config.json">detection_config.json</a></p>
<ul class="simple">
<li><p>Sample Image</p></li>
</ul>
<div class="figure align-default">
<img alt="../_images/image8.jpg" src="../_images/image8.jpg" />
</div>
<p>Once you download the custom object detection model file, you should copy the model file to the your project folder where your <strong>.py</strong> files will be. Then create a python file and give it a name; an example is <strong>FirstCustomDetection.py</strong>. Then write the code below into the python file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imageai.Detection.Custom</span> <span class="kn">import</span> <span class="n">CustomObjectDetection</span>

<span class="n">detector</span> <span class="o">=</span> <span class="n">CustomObjectDetection</span><span class="p">()</span>
<span class="n">detector</span><span class="o">.</span><span class="n">setModelTypeAsYOLOv3</span><span class="p">()</span>
<span class="n">detector</span><span class="o">.</span><span class="n">setModelPath</span><span class="p">(</span><span class="s2">&quot;hololens-ex-60--loss-2.76.h5&quot;</span><span class="p">)</span>
<span class="n">detector</span><span class="o">.</span><span class="n">setJsonPath</span><span class="p">(</span><span class="s2">&quot;detection_config.json&quot;</span><span class="p">)</span>
<span class="n">detector</span><span class="o">.</span><span class="n">loadModel</span><span class="p">()</span>
<span class="n">detections</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">detectObjectsFromImage</span><span class="p">(</span><span class="n">input_image</span><span class="o">=</span><span class="s2">&quot;holo1.jpg&quot;</span><span class="p">,</span> <span class="n">output_image_path</span><span class="o">=</span><span class="s2">&quot;holo1-detected.jpg&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">detections</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">detection</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">],</span> <span class="s2">&quot; : &quot;</span><span class="p">,</span> <span class="n">detection</span><span class="p">[</span><span class="s2">&quot;percentage_probability&quot;</span><span class="p">],</span> <span class="s2">&quot; : &quot;</span><span class="p">,</span> <span class="n">detection</span><span class="p">[</span><span class="s2">&quot;box_points&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>When you run the code, it will produce a result similar to the one below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hololens</span>  <span class="p">:</span>  <span class="mf">39.69653248786926</span>  <span class="p">:</span>  <span class="p">[</span><span class="mi">611</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">751</span><span class="p">,</span> <span class="mi">154</span><span class="p">]</span>
<span class="n">hololens</span>  <span class="p">:</span>  <span class="mf">87.6643180847168</span>  <span class="p">:</span>  <span class="p">[</span><span class="mi">23</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">79</span><span class="p">]</span>
<span class="n">hololens</span>  <span class="p">:</span>  <span class="mf">89.25175070762634</span>  <span class="p">:</span>  <span class="p">[</span><span class="mi">191</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">243</span><span class="p">,</span> <span class="mi">95</span><span class="p">]</span>
<span class="n">hololens</span>  <span class="p">:</span>  <span class="mf">64.49641585350037</span>  <span class="p">:</span>  <span class="p">[</span><span class="mi">437</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="mi">514</span><span class="p">,</span> <span class="mi">133</span><span class="p">]</span>
<span class="n">hololens</span>  <span class="p">:</span>  <span class="mf">91.78624749183655</span>  <span class="p">:</span>  <span class="p">[</span><span class="mi">380</span><span class="p">,</span> <span class="mi">113</span><span class="p">,</span> <span class="mi">423</span><span class="p">,</span> <span class="mi">138</span><span class="p">]</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/image7.jpg" src="../_images/image7.jpg" />
</div>
<p>See more details below:</p>
<ul>
<li><p><strong>.setModelTypeAsYOLOv3()</strong> , This specifies that you are using a trained YOLOv3 model</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">detector</span><span class="o">.</span><span class="n">setModelTypeAsYOLOv3</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p><strong>.setModelPath()</strong> , This is used to set the file path to your trained model</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">detector</span><span class="o">.</span><span class="n">setModelPath</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<p>– <em>parameter</em> <strong>detection_model_path</strong> (required) : This is path to your model file</p>
<ul>
<li><p><strong>.setJsonPath()</strong> , This is used to set the file path to your configuration json file</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">detector</span><span class="o">.</span><span class="n">setJsonPath</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<p>– <em>parameter</em> <strong>configuration_json</strong> (required) : This is path to <em>detection_json</em> file</p>
<ul>
<li><p><strong>.loadModel()</strong> , This is load the detection model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">detector</span><span class="o">.</span><span class="n">loadModel</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p><strong>.detectObjectsFromImage()</strong> , This is the function that performs object detection task after the model as loaded. It can be called many times to detect objects in any number of images. Find example code below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">detections</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">detectObjectsFromImage</span><span class="p">(</span><span class="n">input_image</span><span class="o">=</span><span class="s2">&quot;image.jpg&quot;</span><span class="p">,</span> <span class="n">output_image_path</span><span class="o">=</span><span class="s2">&quot;imagenew.jpg&quot;</span><span class="p">,</span> <span class="n">minimum_percentage_probability</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>– <em>parameter</em> <strong>input_image</strong> (required) : This refers to the path to image file which you want to detect. You can set this parameter to the Numpy array of File stream of any image if you set the paramter <strong>input_type</strong> to “array” or “stream”</p>
<p class="attribution">—<em>parameter</em> <strong>output_image_path</strong> (required only if <strong>input_type</strong> = “file” ) :  This refers to the file path to which the detected image will be saved. It is required only if <strong>input_type</strong> = “file”</p>
</div></blockquote>
<blockquote>
<div><p>– <em>parameter</em> <strong>minimum_percentage_probability</strong> (optional ) :  This parameter is used to determine the integrity of the detection results. Lowering the value shows more objects while increasing the value ensures objects with the highest accuracy are detected. The default value is 50.</p>
<p class="attribution">—<em>parameter</em> <strong>output_type</strong> (optional ) :  This parameter is used to set the format in which the detected image will be produced. The available values are “file” and “array”. The default value is “file”. If this parameter is set to “array”, the function will return a Numpy array of the detected image. See sample below::</p>
</div></blockquote>
<blockquote>
<div><blockquote>
<div><p>returned_image, detections = detector.detectObjectsFromImage(input_image=”image.jpg”, output_type=”array”, minimum_percentage_probability=30)</p>
</div></blockquote>
<p class="attribution">—<em>parameter</em> <strong>display_percentage_probability</strong> (optional ) :  This parameter can be used to hide the percentage probability of each object detected in the detected image if set to False. The default values is True.</p>
</div></blockquote>
<blockquote>
<div><p>– <em>parameter</em> <strong>display_object_name</strong> (optional ) :  This parameter can be used to hide the name of each object detected in the detected image if set to False. The default values is True.</p>
<p class="attribution">—<em>parameter</em> <strong>extract_detected_objects</strong> (optional ) :  This parameter can be used to extract and save/return each object detected in an image as a seperate image. The default values is False.</p>
</div></blockquote>
<blockquote>
<div><p>– <em>parameter</em> <strong>thread_safe</strong> (optional) : This ensures the loaded detection model works across all threads if set to true.</p>
<p class="attribution">—<em>returns</em> :  The returned values will depend on the parameters parsed into the <strong>detectObjectsFromImage()</strong> function. See the comments and code below</p>
</div></blockquote>
<blockquote>
<div><blockquote>
<div><dl class="simple">
<dt>“””</dt><dd><p>If all required parameters are set and ‘output_image_path’ is set to a file path you want the detected image to be saved, the function will return:</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>an array of dictionaries, with each dictionary corresponding to the objects</dt><dd><dl class="simple">
<dt>detected in the image. Each dictionary contains the following property:</dt><dd><ul class="simple">
<li><p>name (string)</p></li>
<li><p>percentage_probability (float)</p></li>
<li><p>box_points (list of x1,y1,x2 and y2 coordinates)</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
</ol>
</dd>
</dl>
<p>“””
detections = detector.detectObjectsFromImage(input_image=”image.jpg”, output_image_path=”imagenew.jpg”, minimum_percentage_probability=30)</p>
<dl class="simple">
<dt>“””</dt><dd><p>If all required parameters are set and output_type = ‘array’ ,the function will return</p>
<ol class="arabic simple">
<li><p>a numpy array of the detected image</p></li>
<li><dl class="simple">
<dt>an array of dictionaries, with each dictionary corresponding to the objects</dt><dd><dl class="simple">
<dt>detected in the image. Each dictionary contains the following property:</dt><dd><ul class="simple">
<li><p>name (string)</p></li>
<li><p>percentage_probability (float)</p></li>
<li><p>box_points (list of x1,y1,x2 and y2 coordinates)</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
</ol>
</dd>
</dl>
<p>“””
returned_image, detections = detector.detectObjectsFromImage(input_image=”image.jpg”, output_type=”array”, minimum_percentage_probability=30)</p>
<dl>
<dt>“””</dt><dd><dl>
<dt>If extract_detected_objects = True and ‘output_image_path’ is set to a file path you want</dt><dd><p>the detected image to be saved, the function will return:
1. an array of dictionaries, with each dictionary corresponding to the objects</p>
<blockquote>
<div><p>detected in the image. Each dictionary contains the following property:
* name (string)
* percentage_probability (float)
* box_points (list of x1,y1,x2 and y2 coordinates)</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>an array of string paths to the image of each object extracted from the image</p></li>
</ol>
</dd>
</dl>
</dd>
</dl>
<p>“””
detections, extracted_objects = detector.detectObjectsFromImage(input_image=”image.jpg”, output_image_path=”imagenew.jpg”, extract_detected_objects=True, minimum_percentage_probability=30)</p>
<dl class="simple">
<dt>“””</dt><dd><dl class="simple">
<dt>If extract_detected_objects = True and output_type = ‘array’, the the function will return:</dt><dd><ol class="arabic simple">
<li><p>a numpy array of the detected image</p></li>
<li><dl class="simple">
<dt>an array of dictionaries, with each dictionary corresponding to the objects</dt><dd><p>detected in the image. Each dictionary contains the following property:
* name (string)
* percentage_probability (float)
* box_points (list of x1,y1,x2 and y2 coordinates)</p>
</dd>
</dl>
</li>
<li><p>an array of numpy arrays of each object detected in the image</p></li>
</ol>
</dd>
</dl>
</dd>
</dl>
<p>“””
returned_image, detections, extracted_objects = detector.detectObjectsFromImage(input_image=”image.jpg”, output_type=”array”, extract_detected_objects=True, minimum_percentage_probability=30)</p>
</div></blockquote>
</div></blockquote>
<p><strong>======= imageai.Detection.Custom.CustomVideoObjectDetection =======</strong></p>
<p><strong>CustomVideoObjectDetection</strong> class provides very convenient and powerful methods to perform object detection on videos and obtain analytical from the video, using your own custom <strong>YOLOv3 model</strong> and the corresponding <strong>detection_config.json</strong> generated during the training.</p>
<p>To test the custom object detection, you can download a sample custom model we have trained to detect the Hololens headset and its <strong>detection_config.json</strong> file via the links below:</p>
<p><a class="reference external" href="https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/hololens-ex-60--loss-2.76.h5">Hololens Detection Model</a></p>
<p><a class="reference external" href="https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/detection_config.json">detection_config.json</a></p>
<p>Download a sample video of the Hololens in the link below.</p>
<p><a class="reference external" href="https://github.com/OlafenwaMoses/ImageAI/blob/master/data-videos/holo1.mp4">Sample Hololens Video</a></p>
<p>Then run the code below in the video:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imageai.Detection.Custom</span> <span class="kn">import</span> <span class="n">CustomVideoObjectDetection</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">execution_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>

<span class="n">video_detector</span> <span class="o">=</span> <span class="n">CustomVideoObjectDetection</span><span class="p">()</span>
<span class="n">video_detector</span><span class="o">.</span><span class="n">setModelTypeAsYOLOv3</span><span class="p">()</span>
<span class="n">video_detector</span><span class="o">.</span><span class="n">setModelPath</span><span class="p">(</span><span class="s2">&quot;hololens-ex-60--loss-2.76.h5&quot;</span><span class="p">)</span>
<span class="n">video_detector</span><span class="o">.</span><span class="n">setJsonPath</span><span class="p">(</span><span class="s2">&quot;detection_config.json&quot;</span><span class="p">)</span>
<span class="n">video_detector</span><span class="o">.</span><span class="n">loadModel</span><span class="p">()</span>

<span class="n">video_detector</span><span class="o">.</span><span class="n">detectObjectsFromVideo</span><span class="p">(</span><span class="n">input_file_path</span><span class="o">=</span><span class="s2">&quot;holo1.mp4&quot;</span><span class="p">,</span>
                                        <span class="n">output_file_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">execution_path</span><span class="p">,</span> <span class="s2">&quot;holo1-detected&quot;</span><span class="p">),</span>
                                        <span class="n">frames_per_second</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                                        <span class="n">minimum_percentage_probability</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                                        <span class="n">log_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/customvideodetection.gif" src="../_images/customvideodetection.gif" />
</div>
<p>See details on the available functions below</p>
<ul>
<li><p><strong>.setModelTypeAsYOLOv3()</strong> , This specifies that you are using a trained YOLOv3 model</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">video_detector</span><span class="o">.</span><span class="n">setModelTypeAsYOLOv3</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p><strong>.setModelPath()</strong> , This is used to set the file path to your trained model</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">video_detector</span><span class="o">.</span><span class="n">setModelPath</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<p>– <em>parameter</em> <strong>detection_model_path</strong> (required) : This is path to your model file</p>
<ul>
<li><p><strong>.setJsonPath()</strong> , This is used to set the file path to your configuration json file</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">video_detector</span><span class="o">.</span><span class="n">setJsonPath</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<p>– <em>parameter</em> <strong>configuration_json</strong> (required) : This is path to <em>detection_json</em> file</p>
<ul>
<li><p><strong>.loadModel()</strong> , This is load the detection model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">video_detector</span><span class="o">.</span><span class="n">loadModel</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p><strong>.detectObjectsFromVideo()</strong> , This is the function that performs object detecttion on a video file or video live-feed after the model has been loaded into the instance you created.  Find a full sample code below:</p></li>
</ul>
<blockquote>
<div><p>– <em>parameter</em> <strong>input_file_path</strong> (required if you did not set <strong>camera_input</strong>) : This refers to the path to the video file you want to detect.</p>
<p class="attribution">—<em>parameter</em> <strong>output_file_path</strong> (required if you did not set <strong>save_detected_video</strong> = False) : This refers to the path to which the detected video will be saved. By default, this functionsaves video  <strong>.avi</strong> format.</p>
</div></blockquote>
<blockquote>
<div><p>– <em>parameter</em> <strong>frames_per_second</strong> (optional , but recommended) : This parameters allows you to set your desired frames per second for the detected video that will be saved. The default value is 20 but we recommend you set the value that suits your video or camera live-feed.</p>
<p class="attribution">—<em>parameter</em> <strong>log_progress</strong> (optional) : Setting this parameter to True shows the progress of the video or live-feed as it is detected in the CLI. It will report every frame detected as it progresses. The default value is False.</p>
</div></blockquote>
<blockquote>
<div><p>– <em>parameter</em> <strong>return_detected_frame</strong> (optional) : This parameter allows you to return the detected frame as a Numpy array at every frame, second and minute of the video detected. The returned Numpy array will be parsed into the respective <strong>per_frame_function</strong>, <strong>per_second_function</strong> and <strong>per_minute_function</strong> (See details below)</p>
<p class="attribution">—<em>parameter</em> <strong>camera_input</strong> (optional) : This parameter can be set in replacement of the <strong>input_file_path</strong> if you want to detect objects in the live-feed of a camera. All you need is to load the camera with OpenCV’s <strong>VideoCapture()</strong> function and parse the object into this parameter.</p>
</div></blockquote>
<blockquote>
<div><p>– <em>parameter</em> <strong>minimum_percentage_probability</strong> (optional ) :  This parameter is used to determine the integrity of the detection results. Lowering the value shows more objects while increasing the value ensures objects with the highest accuracy are detected. The default value is 50.</p>
<p class="attribution">—<em>parameter</em> <strong>display_percentage_probability</strong> (optional ) :  This parameter can be used to hide the percentage probability of each object detected in the detected video if set to False. The default values is True.</p>
</div></blockquote>
<blockquote>
<div><p>– <em>parameter</em> <strong>display_object_name</strong> (optional ) :  This parameter can be used to hide the name of each object detected in the detected video if set to False. The default values is True.</p>
<p class="attribution">—<em>parameter</em> <strong>save_detected_video</strong> (optional ) :  This parameter can be used to or not to save the detected video or not to save it. It is set to True by default.</p>
</div></blockquote>
<blockquote>
<div><p>– <em>parameter</em> <strong>per_frame_function</strong> (optional ) :  This parameter allows you to parse in the name of a function you define. Then, for every frame of the video that is detected, the function will be parsed into the parameter will be executed and and analytical data of the video will be parsed into the function. The data returned can be visualized or saved in a NoSQL database for future processing and visualization.</p>
<blockquote>
<div><dl>
<dt>See a sample function for this parameter below::</dt><dd><p>“””
This parameter allows you to parse in a function you will want to execute after
each frame of the video is detected. If this parameter is set to a function, after every video
frame is detected, the function will be executed with the following values parsed into it:
– position number of the frame
– an array of dictinaries, with each dictinary corresponding to each object detected.</p>
<blockquote>
<div><p>Each dictionary contains ‘name’, ‘percentage_probability’ and ‘box_points’</p>
</div></blockquote>
<dl class="simple">
<dt>– a dictionary with with keys being the name of each unique objects and value</dt><dd><p>are the number of instances of each of the objects present</p>
</dd>
<dt>– If return_detected_frame is set to True, the numpy array of the detected frame will be parsed</dt><dd><p>as the fourth value into the function</p>
</dd>
</dl>
<p>“””</p>
<dl class="simple">
<dt>def forFrame(frame_number, output_array, output_count):</dt><dd><p>print(“FOR FRAME ” , frame_number)
print(“Output for each object : “, output_array)
print(“Output count for unique objects : “, output_count)
print(“————END OF A FRAME ————–”)</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p class="attribution">—<em>parameter</em> <strong>per_second_function</strong> (optional ) :  This parameter allows you to parse in the name of a function you define. Then, for every second of the video that is detected, the function will be parsed into the parameter will be executed and analytical data of the video will be parsed into the function. The data returned can be visualized or saved in a NoSQL database for future processing and visualization.</p>
</div></blockquote>
<blockquote>
<div><blockquote>
<div><dl>
<dt>See a sample function for this parameter below::</dt><dd><p>“””
This parameter allows you to parse in a function you will want to execute after
each second of the video is detected. If this parameter is set to a function, after every second of a video
is detected, the function will be executed with the following values parsed into it:
– position number of the second
– an array of dictionaries whose keys are position number of each frame present in the last second , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame</p>
<p>– an array of dictionaries, with each dictionary corresponding to each frame in the past second, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame</p>
<p>– a dictionary with its keys being the name of each unique object detected throughout the past second, and the key values are the average number of instances of the object found in all the frames contained in the past second</p>
<p>– If return_detected_frame is set to True, the numpy array of the detected frame will be parsed as the fifth value into the function
“””</p>
<dl class="simple">
<dt>def forSeconds(second_number, output_arrays, count_arrays, average_output_count):</dt><dd><p>print(“SECOND : “, second_number)
print(“Array for the outputs of each frame “, output_arrays)
print(“Array for output count for unique objects in each frame : “, count_arrays)
print(“Output average count for unique objects in the last second: “, average_output_count)
print(“————END OF A SECOND ————–”)</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p class="attribution">—<em>parameter</em> <strong>per_minute_function</strong> (optional ) :  This parameter allows you to parse in the name of a function you define. Then, for every frame of the video that is detected, the function which was parsed into the parameter will be executed and analytical data of the video  will be parsed into the function. The data returned has the same nature as the <strong>per_second_function</strong> ; the difference is that it covers all the frames in the past 1 minute of the video.</p>
</div></blockquote>
<blockquote>
<div><blockquote>
<div><p>See a sample function for this parameter below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forMinute</span><span class="p">(</span><span class="n">minute_number</span><span class="p">,</span> <span class="n">output_arrays</span><span class="p">,</span> <span class="n">count_arrays</span><span class="p">,</span> <span class="n">average_output_count</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MINUTE : &quot;</span><span class="p">,</span> <span class="n">minute_number</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array for the outputs of each frame &quot;</span><span class="p">,</span> <span class="n">output_arrays</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array for output count for unique objects in each frame : &quot;</span><span class="p">,</span> <span class="n">count_arrays</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output average count for unique objects in the last minute: &quot;</span><span class="p">,</span> <span class="n">average_output_count</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------END OF A MINUTE --------------&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p class="attribution">—<em>parameter</em> <strong>video_complete_function</strong> (optional ) :  This parameter allows you to parse in the name of a function you define. Once all the frames in the video is fully detected, the function will was parsed into the parameter will be executed and analytical data of the video  will be parsed into the function. The data returned has the same nature as the <strong>per_second_function</strong> and <strong>per_minute_function</strong> ; the differences are that no index will be returned and it covers all the frames in the entire video.</p>
</div></blockquote>
<blockquote>
<div><blockquote>
<div><p>See a sample funtion for this parameter below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forFull</span><span class="p">(</span><span class="n">output_arrays</span><span class="p">,</span> <span class="n">count_arrays</span><span class="p">,</span> <span class="n">average_output_count</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array for the outputs of each frame &quot;</span><span class="p">,</span> <span class="n">output_arrays</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array for output count for unique objects in each frame : &quot;</span><span class="p">,</span> <span class="n">count_arrays</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output average count for unique objects in the entire video: &quot;</span><span class="p">,</span> <span class="n">average_output_count</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------END OF THE VIDEO --------------&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p class="attribution">—<em>parameter</em> <strong>detection_timeout</strong> (optional) :  This function allows you to state the number of seconds of a video that should be detected after which the detection function stop processing the video.</p>
</div></blockquote>
<div class="toctree-wrapper compound">
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../custom/index.html" class="btn btn-neutral float-left" title="Custom Training: Prediction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019, Moses Olafenwa and John Olafenwa

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>